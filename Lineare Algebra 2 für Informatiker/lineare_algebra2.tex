\documentclass[10pt,a4paper]{article}
\author{Jannik Koch}
\title{Lineare Algebra 2 für Informatiker}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{hyperref}

\def\realnumbers{{\rm I\!R}}
\def\naturalnumbers{{\rm I\!N}}
\def\complexnumbers{{\mathbb{C}}}

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\norm}[1]{\lVert#1\rVert}

\begin{document}
	\pagenumbering{Roman}
	{\let\newpage\relax\maketitle}
	\tableofcontents
	\newpage
	\pagenumbering{arabic}
	\setcounter{page}{1}

	\section{Jordan Normalform}
	\label{jd:sec:jordan_normalform}

	Seien $A \in \realnumbers^{N\times N}$, $\lambda \in \realnumbers$.
	
	\subsection{Berechnung der Normalform}
	\label{jd:sub:berechnung_der_normalform}
	
	\begin{itemize}
		\item Berechnung des \textbf{charakteristischen Polynoms} $p_A(\lambda) = |A - \lambda I_N| = 0$
		\item \textbf{Algebraische Vielfachheit} eines Eigenwerts $r_\lambda$: Potenz des Eigenwerts in $p_A(\lambda)$
		\item \textbf{Geometrische Vielfachheit} eines Eigenwerts $d_\lambda$: Dimension des zugehörigen Eigenraums
		\item Hauptdiagonale der Jordan-Normalform $J_A$ entspricht Eigenwerten in beliebiger Reihenfolge, jeder davon jeweils so oft wie $r_\lambda$ vorgibt
		\item $d_\lambda$ gibt die Anzahl der Jordankästchen an
		\item Falls die Anzahl der Jordankästchen keine eindeutige Form impliziert
			\begin{enumerate}
				\item Hauptraum bilden (Eigenraum verallgemeinern)
				\item $q$ entspricht der Länge des längsten Jordankästchens
				\item Ansonsten: Anzahl der Kästchen der Länge k = $dim K_k(\lambda) - dim K_{k - 1}(\lambda)$
			\end{enumerate}
	\end{itemize}
	
	\subsection{Eigenräume verallgemeinern}
	\label{jd:sub:eigenraeume_verallgemeinern}
	
	Annahme: Dimension des Eigenraums ist kleiner als die algebraische Vielfachheit. Für die Jordan-Normalform wird ein Eigenraum zum Hauptraum erweitert.
	\begin{itemize}
		\item $K_k(\lambda) = ker(A - \lambda I_N)^k$, d.h. wir bilden $A - \lambda I_N$ jeweils $k$-mal auf sich selber ab und bilden den Kern für $K_k$; der Eigenraum selber entspricht somit $K_1(\lambda)$
		\item Sobald $K_q(\lambda) = K_{k + 1}(\lambda)$ für ein $q \in \naturalnumbers$ heißt $K_q(\lambda)$ Hauptraum, danach ergeben sich keine neuen Änderungen mehr
	\end{itemize}
	
	\subsection{Minimalpolynom bestimmen}
	\label{jd:sub:minimalpolynom_bestimmen}
	
	\begin{enumerate}
		\item Charakteristisches Polynom bestimmen, Eigenwerte ablesen
		\item Haupträume der Eigenwerte bestimmen
		\item Exponenten des char. Polynoms anpassen für Minimalpolynom $m_A = (X - \lambda_1)^{q_1} ... (X - \lambda_n)^{q_n}$
	\end{enumerate}
	\textbf{Cayley-Hamilton}: $m_A(A) = 0$ und $m_A|p_A$

	\newpage
	\subsection{Jordan-Basis}
	\label{jd:sub:jordan_basis}
	
	Ziel: Finden einer Matrix S, sodass $J_A = S^{-1} * A * S$
	\begin{itemize}
		\item \textbf{Ansatz}: $S = (b_1 | b_2 | \dots | b_n)$ mit Basisvektoren $b_i, i = 1 \dots n$, bzgl. derer die Jordan-Normalform $J_A$ die Abbildungsmatrix von $\phi_A$ ist
		\item \textbf{Vorgehen}:
		\begin{enumerate}
			\item Wähle für jedes Jordankästchen eines Jordanblocks einen Vektor $b_i$ (q sei der \textbf{Index des Hauptraumes}) $$b_i \in Kern(A - \lambda I)^q \setminus Kern(A - \lambda I)^{q-1}$$
			\item Für die restlichen Plätze im Jordankästchen, \textbf{bilde} $b_i$ \textbf{immer wieder ab}: $$b_{j+1} = (A - \lambda I)b_j, j \in \naturalnumbers$$
			\item Sobald dies für jedes Jordan-Kästchen erledigt ist, fülle S so mit den Basisvektoren, dass die Basisvektoren eines Kästchens an den \textbf{zugehörigen Stellen in S} stehen
			\item \textbf{Achtung}: Die Richtung, in der die Basisvektoren für jedes Jordan-Kästchen eingefügt werden, ist davon abhängig, ob die $1$-Elemente der Nebendiagonalen \textbf{rechts oder links} von der Hauptdiagonalen stehen ($i \in \naturalnumbers$)
			\begin{itemize}
				\item Falls \textbf{rechts}: Fülle Kästchen von \textbf{rechts nach links} ($S = (\dots | b_{i+2} | b_{i+1} | b_{i} | \dots$)
				\item Falls \textbf{links}: Fülle Kästchen von \textbf{links nach rechts} ($S = (\dots | b_{i} | b_{i+1} | b_{i+2} | \dots$)
			\end{itemize}
		\end{enumerate}
	\end{itemize}

	\subsection{Tricks \& Hinweise}
	\label{jd:sub:tricks}
	
	\begin{itemize}
		\item $dim(ker \phi)$ bzw. $dim(ker A)$ impliziert die Dimension des Eigenraumes zum Eigenwert 0
		\item $Spur(A)$ = Summe der Eigenwerte mit Vielfachheit (z.B Eigenwert mit algebraischer Vielfachheit 2 zählt doppelt) bei diagonalisierbaren Matrizen (z.B bei einer Matrix aus $\complexnumbers$)
		\item Zwei Matrizen sind genau dann ähnlich, wenn sie (bis auf die Reihenfolge der Jordanblöcke) dieselbe Jordan-Normalform besitzen
		\item (Bei linearer Abbildung, für $A \in K^{N\times N}$:)\\Rang(A) = dim(Bild(A)) und damit dim(ker(A)) = N - dim(Bild(A)) = N - Rang(A)
		\item Die Abbildung ist injektiv, wenn 0 kein Eigenwert ist
	\end{itemize}
	
	\newpage
	\section{Skalarprodukte}
	\label{sp:sec:skalarprodukte}
	
	Sei K ein Körper und V ein K-Vektorraum der Dimension N, $v, v_1, v_2, w, w_1, w_2 \in V$, $\alpha \in K$ sowie
	\begin{center}
		$s(\cdot,\cdot) = \langle\cdot,\cdot\rangle: V \times V \rightarrow \realnumbers^N, (v, w) \mapsto \langle v, w\rangle$
	\end{center}
	Sei weiter $B = \{b_1, ..., b_n\}$ eine Basis von V.
	
	\subsection{Skalarprodukte und Fundamentalmatrizen}
	\label{sp:sub:skalarprodukte_und_fundamentalmatrizen}
	
	Definition einer Fundamentalmatrix:
	\begin{center}
		$F(s) = \begin{bmatrix}
		\langle b_1, b_1\rangle & \langle b_1, b_2\rangle & ... & \langle b_1, b_n\rangle\\
		\vdots & \vdots & \ddots & \vdots \\
		\langle b_n, b_1\rangle & \langle b_n, b_2\rangle & ... & \langle b_n, b_n\rangle\\			
		\end{bmatrix}$
	\end{center}
	Voraussetzungen eines Skalarprodukts (dies gelte für beliebig gewählte Vektoren):
	\begin{itemize}
		\item \textbf{Bilinearform}
		\begin{itemize}
			\item $\langle \alpha v_1 + v_2, w\rangle = \alpha\langle  v_1, w\rangle + \langle v_2, w\rangle$ \textbf{und}
			\item $\langle v, \alpha w_1 + w_2\rangle = \alpha\langle  v, w_1\rangle + \langle v, w_2\rangle$
		\end{itemize}
		\item \textbf{Symmetrisch}
		\begin{itemize}
			\item Formal: $\langle v, w\rangle = \langle w, v\rangle$ \textbf{oder}
			\item Anhand der Fundamentalmatrix: Symmetrisch
		\end{itemize}
		\item \textbf{Positiv-definit}
		\begin{itemize}
			\item Formal: $\langle v, v\rangle \geq 0$ für alle $v \in V$ und $\langle v, v\rangle = 0 \Leftrightarrow v = 0$ \textbf{oder}
			\item Anhand der Fundamentalmatrix: Alle Hauptminoren sind $> 0$,\\alternativ: $v^TF(s)v \geq 0$ für alle $v \in V$
		\end{itemize}
	\end{itemize}
	\textbf{Standardskalarprodukt}: $\langle v, w\rangle = v^Tw$\\\\
	\textbf{Cauchy-Schwarz-Ungleichung}: $\langle v, w\rangle^2 \leq \langle v, v\rangle * \langle w, w\rangle$, Gleichheit genau dann, wenn v und w linear abhängig sind
	
	\subsection{Normen}
	\label{sp:sub:normen}
	
	Sei K $\in \{\realnumbers,\complexnumbers\}$ ein Körper und V ein K-Vektorraum, $v, w \in V$ und $\lambda \in K$ beliebig und eine Abbildung:
	\begin{center}
		$|\cdot|: V \rightarrow \realnumbers$
	\end{center}
	Dann heißt $|\cdot|$ \textbf{Norm}, wenn folgende Bedingungen gelten:
	\begin{enumerate}
		\item \textbf{Positiv definit}: $\norm{v} \geq 0$ und $\norm{v} = 0 \Leftrightarrow v = 0$
		\item \textbf{Homogen}: $\norm{\lambda v} = \lambda \norm{v}$
		\item \textbf{Dreiecksungleichung}: $\norm{v + w} \leq \norm{v} + \norm{w}$
	\end{enumerate}
	\textbf{Standardnorm}: $\norm{v} := \sqrt{\langle v, v\rangle}$
	
	\subsection{Abstandsfunktionen / Metriken}
	\label{sp:sub:abstandsfunktionen_metriken}

	Sei K $\in \{\realnumbers,\complexnumbers\}$ ein Körper und V ein K-Vektorraum, $v, w, z \in V$ beliebig und eine Abbildung:
	\begin{center}
		$d: V \rightarrow \realnumbers$
	\end{center}
	Dann heißt $d$ \textbf{Metrik}, wenn folgende Bedingungen gelten:
	\begin{enumerate}
		\item \textbf{Definitheit}: $d(v, w) > 0$ falls $v \neq w$ und $d(v, w) = 0$ falls $v = w$
		\item \textbf{Symmetrie}: $d(v, w) = d(w, v)$
		\item \textbf{Dreiecksungleichung}: $d(v, z) \leq d(v, w) + d(w, z)$
	\end{enumerate}
	\textbf{Standardmetrik}: $d(v, w) := \norm{w - v} = \sqrt{\langle w - v, w - v\rangle}$
		
	\subsection{Winkel}
	\label{sp:sub:winkel}
	
	Sei V ein euklidischer Vektorraum (Vektorraum über $\realnumbers$ mit Skalarprodukt); das Skalarprodukt sei $\langle \cdot, \cdot\rangle$ und $v, w\in V$ beliebig. So gilt für den Winkel zwischen v und w:
	\begin{center}
		$cos(\alpha) := cos(\sphericalangle(v, w)) = \frac{\langle v, w\rangle}{\norm{v}\norm{w}} = \frac{\langle v, w\rangle}{\sqrt{\langle v, v \rangle}\sqrt{\langle w, w\rangle}}$
	\end{center}
	Ist das \textbf{Skalarprodukt gleich 0}, so sind die Vektoren \textbf{orthogonal}.

	\subsection{Änderungen bei unitären Vektorräumen}
	\label{sp:sub:aenderungen_bei_unitaeren_vektorraeumen}
	
	Ist der zugrunde liegende Vektorraum unitär (d.h. K ist $\complexnumbers$) so muss für das Skalarprodukt gelten:
	\begin{itemize}
		\item Statt einer Bilinearform ist das Skalarprodukt eine \textbf{Sesquilinearform}:
		\begin{itemize}
			\item $\langle \alpha v_1 + v_2, w\rangle = \alpha\langle  v_1, w\rangle + \langle v_2, w\rangle$ \textbf{und}
			\item $\langle v, \alpha w_1 + w_2\rangle = \overline{\alpha}\langle  v, w_1\rangle + \langle v, w_2\rangle$
		\end{itemize}
		\item Statt symmetrisch ist das Skalarprodukt \textbf{hermitesch}:
			\begin{itemize}
				\item Formal: $\langle v, w\rangle = \overline{\langle w, v\rangle}$ \textbf{oder}
				\item Anhand der Fundamentalmatrix: $F(s) = \overline{F}^T$
			\end{itemize}
		\item Das Skalarprodukt muss weiterhin \textbf{positiv definit} sein wie bisher
	\end{itemize}
	\textbf{Standardskalarprodukt}: $\langle v, w\rangle := v^T\overline{w} = \sum_{i = 1}^{n} v_i\overline{w}_i$\\\\
	\textbf{Unitäre Cauchy-Schwarz-Ungleichung}: $|\langle v, w\rangle|^2 \leq \langle v, v\rangle * \langle w, w\rangle$, Gleichheit genau dann, wenn v und w linear abhängig sind
	
	\subsection{Tricks \& Hinweise}
	\label{sp:sub:tricks_hinweise}
	
	\begin{itemize}
		\item \textbf{Fourierformel}: B Orthonormalbasis, $v \in V \Rightarrow v = \sum_{b \in B} \langle v, b\rangle * b$
		\item Für eine quadratische Matrix A gilt: $|det(A)|^2 = det(A) * \overline{det(A)} = det(A) * det(\overline{A})$ sowie $det(A) = det(A^T)$
		\item Eine Bilinearform heißt nicht ausgeartet, wenn
		\begin{itemize}
			\item für alle $v \in V$ ein $w \in W$ existiert mit $\langle v, w\rangle \neq 0$
			\item für alle $w \in W$ ein $v \in V$ existiert mit $\langle v, w\rangle \neq 0$
		\end{itemize}
	\end{itemize}

	\newpage
	\section{Orthogonalsysteme}
	\label{os:sec:orthogonalsysteme}
	
	Sei K ein Körper und V ein K-Vektorraum mit Basis $B = \{b_1, ..., b_n\}$ sowie einem Skalarprodukt $\langle \cdot, \cdot\rangle$ und einer Norm $|\cdot|$.
	
	\subsection{Gram-Schmidt Orthogonalisierung und -normalisierung}
	\label{os:sub:gram_schmidt_orthogonalisierung_und_normalisierung}
	
	Ziel: Ermittlung einer Basis aus orthogonalen Vektoren bzw. orthonormalen Vektoren (Vektoren haben Länge 1, wie z.B die Standardbasisvektoren bzgl. der Standardmetrik).\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Wähle Startvektor $b^*_1 = b_1$ (und normiere diesen für eine Orthonormalbasis mit $c_1 =  \frac{b^*_1}{\norm{b^*`_1}}$)
		\item Iteriere durch die restlichen Basisvektoren und berechne
		\begin{enumerate}
			\item Abziehen der parallelen Teile der vorherigen Basisvektoren für eine Orthogonalbasis:\\$b^*_i = b_i - \sum_{j = 1}^{i - 1} \frac{\langle b_i, c_j\rangle}{\langle c_j, c_j\rangle}c_j$
			\item Normierung: $c_i = \frac{b^*_i}{\norm{b^*_i}}$
		\end{enumerate}
	\end{enumerate}
	Damit ergibt sich die Orthogonalbasis bzgl. des Skalarprodukts und der Norm: $B^* = \{b^*_1, ..., b^*_n\}$ sowie (optional) die Orthonormalbasis $C = \{c_1, ..., c_n\}$.
	
	\subsection{Orthogonale Komplemente und Projektionen}
	\label{sub:orthogonale_komplemente_und_projektionen}
	
	\textbf{Allgemein:}
	\begin{itemize}
		\item Das \textbf{orthogonale Komplement} einer Teilmenge M des Vektorraums V entspricht:
		\begin{center}
			$M^\perp := \{v \in V\ |\ v\perp m\ \forall m\in M\} = \{v\in V\ |\ \langle v, m\rangle = 0\ \forall m\in M\}$
		\end{center}
		\item Es gilt für einen Untervektorraum U zu V:
		\begin{center}
			$V = U \bigoplus U^\perp$ und $dim V = dim U + dim U^\perp$
		\end{center}
		\item Weiter gibt es die \textbf{orthogonale Projektion}, die das orthogonale Komplement unterschlägt:
		\begin{center}
			$\Pi_U: V = U \bigoplus U^\perp \rightarrow U, v = u + u^\perp \mapsto u$
		\end{center}
		wobei $ker \Pi_U = U^\perp,\ \Pi_{U|U} = id_U$ und $\Pi^2_U = \Pi_U$
		\item $\Pi_U(v)$ ist der \textbf{Lotfußpunkt} von v auf U und hat in U den kürzesten Abstand zu v
	\end{itemize}

	\newpage
	\noindent\textbf{Berechnung der orthogonalen Projektion:}
	\begin{enumerate}
		\item Berechne die \textbf{Orthonormalbasis} $\{c_1, ..., c_m\}$ \textbf{von U}
		\item Ergänze diese zu einer \textbf{Orthonormalbasis von V}, d.h. $\{c_1, ..., c_m, c_{m + 1}, ... c_n\}$, der ergänzte Teil ist die Orthonormalbasis zu $U^\perp$
		\item Damit folgt die orth. Projektion: $\Pi_U: V \rightarrow U, b_i \mapsto
		\begin{cases}
			b_i,& falls\ i \in \{1, ..., m\}\\
			0,& falls\ i \in \{m + 1, ..., n\}\\
		\end{cases}$
	\end{enumerate}
	Für die \textbf{direkte Berechnung} der Orthogonalprojektion eines Vektors v aus V auf U gilt mit der Orthonormalbasis $\{c_1, ..., c_m\}$ von U:
	\begin{center}
		$\Pi_U(v) = \sum_{i = 1}^{n} \langle v, c_i \rangle c_i$
	\end{center}
	\textbf{Berechnung des Abstands}:
	\begin{itemize}
		\item Für den Abstand eines Vektors v zum Untervektorraum U gilt:
		\begin{center}
			$d(v, U) = d(v, \Pi_U(v)) = ||v - \Pi_U(v)||$
		\end{center}
		\item Damit: Lotfußpunkt ausrechnen und Abstand per gegebener Norm ausrechnen
	\end{itemize}

	\subsection{Orthogonale und Unitäre Gruppen}
	\label{os:sub:orthogonale_und_unitaere_gruppen}
	
	\begin{itemize}
		\item \textbf{Orthogonale Gruppe}: $O(n) := \{A \in \realnumbers^{N\times N}\ |\ A^TA = I_N\}$
		\item \textbf{Spezielle orthogonale Gruppe}: $SO(n) := \{A \in O(n)\ |\ det A = 1\}$
		\item \textbf{Unitäre Gruppe}: $U(n) := \{A \in \complexnumbers^{N\times N}\ |\ \overline{A}^TA = I_N\}$
		\item \textbf{Spezielle unitäre Gruppe}: $SU(n) := \{A \in U(n)\ |\ det A = 1\}$
	\end{itemize}

	\subsection{Iwasawa-Zerlegung}
	\label{os:sub:iwasawa_zerlegung}
	
	\textbf{Ziel}: Zerlegen einer Matrix $A \in \realnumbers^{N\times N} (\complexnumbers^{N\times N})$ in das Produkt einer orthogonalen (unitären) Matrix O und einer oberen Dreiecksmatrix R mit positiven reellen Diagonaleinträgen, sodass: $A = OR$.\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Bilde eine \textbf{Orthonormalbasis} $C = \{c_1, ..., c_n\}$ aus den Basisvektoren in den Spalten der Matrix A ($A = (a_1 | a_2 | ... | a_n)$)
		\item O entspricht den \textbf{Basisvektoren der Orthonormalbasis} $c_i$, d.h. $O = (c_1 | c_2 | ... | c_n)$
		\item Bilde R über \textbf{Gauß-Jordan}, d.h. entwickle A nach O, d.h. löse das LGS $(O | A)$, sodass die linke Matrix $I_N$ entspricht; die \textbf{rechte Matrix entspricht dann R}
	\end{enumerate}
	
	\newpage
	\section{Isometrien}
	Seien V, W euklidische (unitäre) Vektorräume und $\{c_1, ..., c_n\}$ eine Orthonormalbasis von V sowie $\phi: V \rightarrow W$ eine lineare Abbildung und $v, w \in V$ beliebig.
	
	\subsection{Isometrien}
	\label{os:sub:isometrien}	

	\textbf{Definition}: Längenerhaltende Abbildung zwischen metrischen Räumen; folgende Aussagen zu Isometrien sind äquivalent:
	\begin{itemize}
		\item $\phi$ ist eine Isometrie
		\item $\langle v, w\rangle = \langle \phi(v), \phi(w)\rangle$
		\item $\langle \phi(e), \phi(e)\rangle = 1$ für alle Einheitsvektoren $e \in V$
		\item $\{\phi(c_1), \phi(c_2), ..., \phi(c_n)\}$ ist eine Orthonormalbasis von W
		\item Falls V = W: Die Abbildungsmatrix $D_{CC}(\phi)$ ist orthogonal (unitär)
	\end{itemize}
	\textbf{Weiter gilt für alle Isometrien:}
	\begin{itemize}
		\item Jeder Eigenwert von $\phi$ hat Betrag 1
		\item $|det \phi| = 1$
		\item In einem n-dimensionalen Raum lässt sich jede lineare Isometrie als Verknüpfung von maximal n Spiegelungen schreiben
	\end{itemize}

	\subsection{(Euklidische) Isometrie-Normalform}
	\label{os:sub:euklidische_isometrie_normalform}

	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Aufstellen des char. Polynoms, Berechnung der Eigenwerte
		\begin{itemize}
			\item Ist die Matrix unitär oder euklidisch und diagonalisierbar, zerfällt das char. Polynom in Linearfaktoren, die Diagonalmatrix ist die Normalform und wir sind fertig
		\end{itemize}
		\item Ansonsten: Aufstellen einer Matrix mit Eigenwerten oder Drehkästchen auf der Diagonalen
		\begin{itemize}
			\item Char. Polynom zerfällt in bekannte \textbf{Linearfaktoren}, für welche nur der Eigenwert auf der Hauptdiagonalen eingetragen wird (z.B $(X + 1)$) und kompliziertere Faktoren (z.B $(X^2 - \frac{49}{25}X + 1)$), welche \textbf{Drehkästchen} ergeben
			\item \textbf{Bestimmung von Drehkästchen}: Lösen der quadr. Gleichung über Lösungsformel, Ergebnis hat komplexe Form (hier: $\frac{49}{50} + \frac{3}{50}\sqrt{11}i$), Definition dieser Form als
			\begin{center}
				$cos(\omega) + isin(\omega)$
			\end{center}
			und eintragen als Drehkästchen D mit:
		\end{itemize}
	\end{enumerate}
		\begin{center}
		$D = \begin{bmatrix}
			cos(\omega) & -sin(\omega)\\
			sin(\omega) & cos(\omega)\\
		\end{bmatrix}
	\overrightarrow{Normalform}
		\begin{bmatrix}
			-1 & 0 & 0 \\
			0 & cos(\omega) & -sin(\omega)\\
			0 & sin(\omega) & cos(\omega)\\
		\end{bmatrix}$
	\end{center}
	
	\subsection{Ähnlichkeitstransformation der Isometrie-Normalform}
	\label{os:sub:aehnlichkeitstransformation_der_isometrie_normalform}
	
	\begin{itemize}
		\item \textbf{Ziel}: Analog zur Jordan-Normalform: Finden einer (orthogonalen) Transformationsmatrix $S = (b_1 | \dots | b_n)$, sodass $S^TAS = A_{INF}$
		\item \textbf{Vorgehen für (-)1-Kästchen}:
		\begin{enumerate}
			\item Finde \textbf{Eigenvektoren analog zu Jordan}
			\item Die die Basisvektoren für diese Stellen in $S$ sind die \textbf{normierten Eigenvektoren}
		\end{enumerate}
		\item \textbf{Vorgehen für Drehkästchen}:
		\begin{enumerate}
			\item Finde einen Vektor \textbf{orthogonal zu den anderen Basisvektoren}
			\item \textbf{Bilde diesen mit A ab}, bis genug Vektoren für die Stellen gefunden sind
			\item Berechne mit Gram-Schmidt aus diesen Basisvektoren eine \textbf{Orthonormalbasis}
			\item Die orthonormalen Vektoren sind die Basisvektoren für die Stellen des Drehkästchens\\in $S$
		\end{enumerate}
	\end{itemize}

	\newpage
	\section{Adjungierte und normale Abbildungen}
	\label{aa:sec:adjungierte_und_normale_abbildungen}
	
	Sei $K \in \{\realnumbers, \complexnumbers\}$, V, W seien K-Vektorräume sowie $\phi: V \rightarrow W$ eine lineare Abbildung. Die Vektorräume haben Skalarprodukte $\langle \cdot, \cdot\rangle_V$ und $\langle \cdot, \cdot\rangle_W$ sowie Orthonormalbasen $C_V, C_W$.
	
	\subsection{Adjungierte Abbildungen}
	\label{aa:sub:adjungierte_abbildungen}

	Eine Abbildung $\phi^*: W \rightarrow V$ heißt \textbf{adjungiert} zu $\phi$, falls für alle $v, w \in V$ gilt:
	\begin{center}
		$\langle \phi(v), w\rangle = \langle v, \phi^*(w)\rangle$
	\end{center}
	\textbf{Eigenschaften}:
	\begin{itemize}
		\item Existiert eine zu $\phi$ adjungierte Abbildung, so ist diese \textbf{eindeutig}
		\item Sind V und W endlichdimensional, so existiert $\phi^*$
		\item Es gilt: $D_{C_VC_W}(\phi^*) = \overline{D_{C_VC_W}(\phi)}^T$ sowie $(\phi^*)^* = \phi$
		\item Für einen weiteren K-Vektorraum Z mit Skalarprodukt und einer linearen Abbildung\\ $\psi: W \rightarrow Z$ mit adjungierter Abbildung $\phi^*$ gilt: $(\psi \circ \phi)^* = \phi^* \circ \psi^*$
		\item Für jeden Eigenwert $\lambda$ von $\phi$ ist $\overline{\lambda}$ ein Eigenwert von $\phi^*$
	\end{itemize}
	\textbf{Berechnung der adjungierten Abbildung}:\\
	Mit einer Orthonormalbasis $C_V = \{c_1, c_2, ..., c_n\}$ von V gilt:
	\begin{center}
		$v = \sum_{j = 1}^{n}\langle v, c_i\rangle c_i \Rightarrow \phi^*(w) = \sum_{i = 1}^{n}\langle \phi^*(w), c_i\rangle c_i = \sum_{i = 1}^{n}\langle w, \phi(c_i)\rangle c_i$
	\end{center}
	
	\subsection{Selbstadjungierte Homomorphismen}
	\label{aa:sub:selbstadjungierte_homomorphismen}

	Selbstadjungierte Homomorphismen sind ein Spezialfall von adjungierten Abbildungen. Sei V ein Vektorraum mit Skalarprodukt, dann ist die Abbildung $\phi: V \rightarrow V$ \textbf{selbstadjungiert}, wenn gilt:
	\begin{center}
		$\langle \phi(v), w\rangle = \langle v, \phi(w)\rangle\ \forall v, w\in V$
	\end{center}
	\textbf{Eigenschaften, wenn eine selbstadjungierte Abbildung existiert}:
	\begin{itemize}
		\item Alle Eigenwerte von $\phi$ sind reell
		\item Das char. Polynom von $\phi$ zerfällt in reelle Linearfaktoren
	\end{itemize}
	$\phi$ ist genau dann selbstadjungiert, wenn $\phi$ \textbf{nur reelle Eigenwerte} und eine \textbf{Orthonormalbasis aus Eigenvektoren} besitzt.

	\subsection{Normale Homomorphismen}
	\label{aa:sub:normale_homomorphismen}

	Sei V ein Vektorraum mit Skalarprodukt und $\phi: V \rightarrow V$ eine lineare Abbildung. $\phi$ heißt \textbf{normal}, falls $\phi^*$ existiert und eine der folgenden Bedingungen erfüllt ist:
	\begin{itemize}
		\item $\phi \circ \phi^* = \phi^* \circ \phi$
		\item $\forall v,w \in V: \langle \phi(v), \phi(w)\rangle = \langle \phi^*(v), \phi^*(w)\rangle$
	\end{itemize}
	Mit einer \textbf{Abbildungsmatrix} $A = D_{CC}(\phi)$ bzgl. einer \textbf{Orthonormalbasis} C von V gilt auch:
	\begin{center}
		$\phi$ ist normal $\Leftrightarrow A \overline{A}^T = \overline{A}^TA$
	\end{center}
\end{document}