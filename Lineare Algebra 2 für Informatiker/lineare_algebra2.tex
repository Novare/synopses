\documentclass[10pt,a4paper]{article}
\author{Jannik Koch}
\title{Lineare Algebra 2 für Informatiker}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\def\realnumbers{{\rm I\!R}}
\def\naturalnumbers{{\rm I\!N}}
\def\complexnumbers{{\mathbb{C}}}

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\norm}[1]{\lVert#1\rVert}

\begin{document}
	\pagenumbering{Roman}
	{\let\newpage\relax\maketitle}
	\tableofcontents
	\newpage
	\pagenumbering{arabic}
	\setcounter{page}{1}

	\section{Jordan Normalform}
	Seien $A \in \realnumbers^{N\times N}$, $\lambda \in \realnumbers$.
	
	\subsection{Berechnung der Normalform}
	\begin{itemize}
		\item Berechnung des \textbf{charakteristischen Polynoms} $p_A(\lambda) = |A - \lambda I_N| = 0$
		\item \textbf{Algebraische Vielfachheit} eines Eigenwerts $r_\lambda$: Potenz des Eigenwerts in $p_A(\lambda)$
		\item \textbf{Geometrische Vielfachheit} eines Eigenwerts $d_\lambda$: Dimension des zugehörigen Eigenraums
		\item Hauptdiagonale der Jordan-Normalform $J_A$ entspricht Eigenwerten in beliebiger Reihenfolge, jeder davon jeweils so oft wie $r_\lambda$ vorgibt
		\item $d_\lambda$ gibt die Anzahl der Jordankästchen an
		\item Falls die Anzahl der Jordankästchen keine eindeutige Form impliziert
			\begin{enumerate}
				\item Hauptraum bilden (Eigenraum verallgemeinern)
				\item $q$ entspricht der Länge des längsten Jordankästchens
				\item Ansonsten: Anzahl der Kästchen der Länge k = $dim K_k(\lambda) - dim K_{k - 1}(\lambda¸)$
			\end{enumerate}
	\end{itemize}
	
	\subsection{Eigenräume verallgemeinern}
	Annahme: Dimension des Eigenraums ist kleiner als die algebraische Vielfachheit. Für die Jordan-Normalform wird ein Eigenraum zum Hauptraum erweitert.
	\begin{itemize}
		\item $K_k(\lambda) = ker(A - \lambda I_N)^k$, d.h. wir bilden $A - \lambda I_N$ jeweils $k$-mal auf sich selber ab und bilden den Kern für $K_k$; der Eigenraum selber entspricht somit $K_1(\lambda)$
		\item Sobald $K_q(\lambda) = K_{k + 1}(\lambda)$ für ein $q \in \naturalnumbers$ heißt $K_q(\lambda)$ Hauptraum, danach ergeben sich keine neuen Änderungen mehr
	\end{itemize}
	
	\subsection{Minimalpolynom bestimmen}
	\begin{enumerate}
		\item Charakteristisches Polynom bestimmen, Eigenwerte ablesen
		\item Haupträume der Eigenwerte bestimmen
		\item Exponenten des char. Polynoms anpassen für Minimalpolynom $m_A = (X - \lambda_1)^{q_1} ... (X - \lambda_n)^{q_n}$
	\end{enumerate}
	\textbf{Cayley-Hamilton}: $m_A(A) = 0$ und $m_A|p_A$

	\subsection{Jordan-Basis}
	Ziel: Finden einer Matrix S, sodass $J_A = S^{-1} * A * S$
	\begin{enumerate}
		\item Für jeden Eigenwert Vektoren $v \in ker(A - \lambda I_N)^q \setminus ker(A - \lambda I_N)^{q - 1}$ wählen;\\diese bilden die rechteste Spalte in S für das entsprechende Jordankästchen
		\item Das Kästchen wird nach links aufgefüllt, indem man v immer wieder abbildet:\\$(A - \lambda I_N)^{q - 1}v, (A - \lambda I_N)^{q - 2}v ... (A - \lambda I_N)v$
		\item Die Reihenfolge muss zur aufgestellten Normalform passen und ist nicht eindeutig!
	\end{enumerate}

	\subsection{Tricks}
		\begin{itemize}
			\item $dim(ker \phi)$ bzw. $dim(ker A)$ impliziert die Dimension des Eigenraumes zum Eigenwert 0
			\item $Spur(A)$ = Summe der Eigenwerte mit Vielfachheit (z.B Eigenwert mit algebraischer Vielfachheit 2 zählt doppelt)
		\end{itemize}
	
	\section{Skalarprodukte}
	Sei K ein Körper und V ein K-Vektorraum der Dimension N, $v, v_1, v_2, w, w_1, w_2 \in V$, $\alpha \in K$ sowie
	\begin{center}
		$s(\cdot,\cdot) = \langle\cdot,\cdot\rangle: V \times V \rightarrow \realnumbers^N, (v, w) \mapsto \langle v, w\rangle$
	\end{center}
	Sei weiter $B = \{b_1, ..., b_n\}$ eine Basis von V.
	
	\subsection{Skalarprodukte und Fundamentalmatrizen}
		Definition einer Fundamentalmatrix:
		\begin{center}
			$F(s) = \begin{bmatrix}
			\langle b_1, b_1\rangle & \langle b_1, b_2\rangle & ... & \langle b_1, b_n\rangle\\
			\vdots & \vdots & \ddots & \vdots \\
			\langle b_n, b_1\rangle & \langle b_n, b_2\rangle & ... & \langle b_n, b_n\rangle\\			
			\end{bmatrix}$
		\end{center}
		Voraussetzungen eines Skalarprodukts (dies gelte für beliebig gewählte Vektoren):
		\begin{itemize}
			\item \textbf{Bilinearform}
			\begin{itemize}
				\item $\langle \alpha v_1 + v_2, w\rangle = \alpha\langle  v_1, w\rangle + \langle v_2, w\rangle$ \textbf{und}
				\item $\langle v, \alpha w_1 + w_2\rangle = \alpha\langle  v, w_1\rangle + \langle v, w_2\rangle$
			\end{itemize}
			\item \textbf{Symmetrisch}
			\begin{itemize}
				\item Formal: $\langle v, w\rangle = \langle w, v\rangle$ \textbf{oder}
				\item Anhand der Fundamentalmatrix: Symmetrisch
			\end{itemize}
			\item \textbf{Positiv-definit}
			\begin{itemize}
				\item Formal: $\langle v, v\rangle \geq 0$ für alle $v \in V$ und $\langle v, v\rangle = 0 \Leftrightarrow v = 0$ \textbf{oder}
				\item Anhand der Fundamentalmatrix: Alle Hauptminoren sind $> 0$,\\alternativ: $v^TF(s)v \geq 0$ für alle $v \in V$
			\end{itemize}
		\end{itemize}
	\textbf{Standardskalarprodukt}: $\langle v, w\rangle = v^Tw$\\\\
	\textbf{Cauchy-Schwarz-Ungleichung}: $\langle v, w\rangle^2 \leq \langle v, v\rangle * \langle w, w\rangle$, Gleichheit genau dann, wenn v und w linear abhängig sind
	
	\subsection{Normen}
	Sei K $\in \{\realnumbers,\complexnumbers\}$ ein Körper und V ein K-Vektorraum, $v, w \in V$ und $\lambda \in K$ beliebig und eine Abbildung:
	\begin{center}
		$|\cdot|: V \rightarrow \realnumbers$
	\end{center}
	Dann heißt $|\cdot|$ \textbf{Norm}, wenn folgende Bedingungen gelten:
	\begin{enumerate}
		\item \textbf{Positiv definit}: $\norm{v} \geq 0$ und $\norm{v} = 0 \Leftrightarrow v = 0$
		\item \textbf{Homogen}: $\norm{\lambda v} = \lambda \norm{v}$
		\item \textbf{Dreiecksungleichung}: $\norm{v + w} \leq \norm{v} + \norm{w}$
	\end{enumerate}
	\textbf{Standardnorm}: $\norm{v} := \sqrt{\langle v, v\rangle}$
	
	\subsection{Abstandsfunktionen / Metriken}
	Sei K $\in \{\realnumbers,\complexnumbers\}$ ein Körper und V ein K-Vektorraum, $v, w, z \in V$ beliebig und eine Abbildung:
	\begin{center}
		$d: V \rightarrow \realnumbers$
	\end{center}
	Dann heißt $d$ \textbf{Metrik}, wenn folgende Bedingungen gelten:
	\begin{enumerate}
		\item \textbf{Definitheit}: $d(v, w) > 0$ falls $v \neq w$ und $d(v, w) = 0$ falls $v = w$
		\item \textbf{Symmetrie}: $d(v, w) = d(w, v)$
		\item \textbf{Dreiecksungleichung}: $d(v, z) \leq d(v, w) + d(w, z)$
	\end{enumerate}
	\textbf{Standardmetrik}: $d(v, w) := \norm{w - v} = \sqrt{\langle w - v, w - v\rangle}$
		
	\subsection{Winkel}
	Sei V ein euklidischer Vektorraum mit Skalarprodukt $\langle \cdot, \cdot\rangle$ und $v, w\in V$ beliebig. So gilt für den Winkel zwischen v und w:
	\begin{center}
		$cos(\alpha) := cos(\sphericalangle(v, w)) = \frac{\langle v, w\rangle}{\norm{v}\norm{w}} = \frac{\langle v, w\rangle}{\sqrt{\langle v, v \rangle}\sqrt{\langle w, w\rangle}}$
	\end{center}
	Ist das \textbf{Skalarprodukt gleich 0}, so sind die Vektoren \textbf{orthogonal}.
	\newpage
	\subsection{Änderungen bei unitären Vektorräumen}
	Ist der zugrunde liegende Vektorraum unitär (d.h. K ist $\complexnumbers$) so muss für das Skalarprodukt gelten:
	\begin{itemize}
		\item Statt einer Bilinearform ist das Skalarprodukt eine \textbf{Sesquilinearform}:
		\begin{itemize}
			\item $\langle \alpha v_1 + v_2, w\rangle = \alpha\langle  v_1, w\rangle + \langle v_2, w\rangle$ \textbf{und}
			\item $\langle v, \alpha w_1 + w_2\rangle = \overline{\alpha}\langle  v, w_1\rangle + \langle v, w_2\rangle$
		\end{itemize}
		\item Statt symmetrisch ist das Skalarprodukt \textbf{hermitesch}:
			\begin{itemize}
				\item Formal: $\langle v, w\rangle = \overline{\langle w, v\rangle}$ \textbf{oder}
				\item Anhand der Fundamentalmatrix: $F(s) = \overline{F}^T$
			\end{itemize}
		\item Das Skalarprodukt muss weiterhin \textbf{positiv definit} sein wie bisher
	\end{itemize}
	\textbf{Standardskalarprodukt}: $\langle v, w\rangle := v^T\overline{w} = \sum_{i = 1}^{n} v_i\overline{w}_i$\\\\
	\textbf{Unitäre Cauchy-Schwarz-Ungleichung}: $|\langle v, w\rangle|^2 \leq \langle v, v\rangle * \langle w, w\rangle$, Gleichheit genau dann, wenn v und w linear abhängig sind
	
	\section{Orthogonalsysteme}
	Sei K ein Körper und V ein K-Vektorraum mit Basis $B = \{b_1, ..., b_n\}$ sowie einem Skalarprodukt $\langle \cdot, \cdot\rangle$ und einer Norm $|\cdot|$.
	
	\subsection{Gram-Schmidt-Orthogonalisierung und -normalisierung}
	Ziel: Ermittlung einer Basis aus orthogonalen Vektoren bzw. orthonormalen Vektoren (Vektoren haben Länge 1, wie z.B die Standardbasisvektoren bzgl. der Standardmetrik).\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Wähle Startvektor $b^*_1 = b_1$ (und normiere diesen für eine Orthonormalbasis mit $c_1 =  \frac{b^*_1}{\norm{b^*`_1}}$)
		\item Iteriere durch die restlichen Basisvektoren und berechne
		\begin{enumerate}
			\item Abziehen der parallelen Teile der vorherigen Basisvektoren für eine Orthogonalbasis:\\$b^*_i = b_i - \sum_{j = 1}^{i - 1} \frac{\langle b_i, c_j\rangle}{\langle c_j, c_j\rangle}c_j$
			\item ggf. Bilden der Orthonormalbasis durch Normieren: $c_i = \frac{b^*_i}{\norm{b^*_i}}$
		\end{enumerate}
	\end{enumerate}
	Damit ergibt sich die Orthogonalbasis bzgl. des Skalarprodukts und der Norm: $B^* = \{b^*_1, ..., b^*_n\}$ sowie (optional) die Orthonormalbasis $C = \{c_1, ..., c_n\}$.
	
	\subsection{Orthogonale Komplemente und Projektionen}
	\textbf{Allgemein:}
	\begin{itemize}
		\item Das \textbf{orthogonale Komplement} einer Teilmenge M des Vektorraums V entspricht:
		\begin{center}
			$M^\perp := \{v \in V\ |\ v\perp m\ \forall m\in M\} = \{v\in V\ |\ \langle v, m\rangle = 0\ \forall m\in M\}$
		\end{center}
		\item Es gilt für einen Untervektorraum U zu V:
		\begin{center}
			$V = U \bigoplus U^\perp$ und $dim V = dim U + dim U^\perp$
		\end{center}
		\item Weiter gibt es die \textbf{orthogonale Projektion}, die das orthogonale Komplement unterschlägt:
		\begin{center}
			$\Pi_U: V = U \bigoplus U^\perp \rightarrow U, v = u + u^\perp \mapsto u$
		\end{center}
		wobei $ker \Pi_U = U^\perp,\ \Pi_{U|U} = id_U$ und $\Pi^2_U = \Pi_U$
		\item $\Pi_U(v)$ ist der \textbf{Lotfußpunkt} von v auf U und hat in U den kürzesten Abstand zu v
	\end{itemize}
	\textbf{Berechnung der orthogonalen Projektion:}
	\begin{enumerate}
		\item Berechne die \textbf{Orthonormalbasis} $\{c_1, ..., c_m\}$ \textbf{von U}
		\item Ergänze diese zu einer \textbf{Orthonormalbasis von V}, d.h. $\{c_1, ..., c_m, c_{m + 1}, ... c_n\}$, der ergänzte Teil ist die Orthonormalbasis zu $U^\perp$
		\item Damit folgt die orth. Projektion: $\Pi_U: V \rightarrow U, b_i \mapsto
		\begin{cases}
			b_i,& falls\ i \in \{1, ..., m\}\\
			0,& falls\ i \in \{m + 1, ..., n\}\\
		\end{cases}$
	\end{enumerate}

	\subsection{Orthogonale und Unitäre Gruppen}
	\begin{itemize}
		\item \textbf{Orthogonale Gruppe}: $O(n) := \{A \in \realnumbers^{N\times N}\ |\ A^TA = I_N\}$
		\item \textbf{Spezielle orthogonale Gruppe}: $SO(n) := \{A \in O(n)\ |\ det A = 1\}$
		\item \textbf{Unitäre Gruppe}: $U(n) := \{A \in \complexnumbers^{N\times N}\ |\ \overline{A}^TA = I_N\}$
		\item \textbf{Spezielle unitäre Gruppe}: $SU(n) := \{A \in U(n)\ |\ det A = 1\}$
	\end{itemize}

	\subsection{Iwasawa-Zerlegung}
	\textbf{Ziel}: Zerlegen einer Matrix $A \in \realnumbers^{N\times N} (\complexnumbers^{N\times N})$ in das Produkt einer orthogonalen (unitären) Matrix O und einer oberen Dreiecksmatrix R mit positiven reellen Diagonaleinträgen, sodass: $A = OR$.\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Bilde eine \textbf{Orthonormalbasis} $C = \{c_1, ..., c_n\}$ aus den Basisvektoren in den Spalten der Matrix A ($A = (a_1 | a_2 | ... | a_n)$)
		\item O entspricht den \textbf{Basisvektoren der Orthonormalbasis} $c_i$, d.h. $O = (c_1 | c_2 | ... | c_n)$
		\item Bilde R über \textbf{Gauß-Jordan}, d.h. entwickle A nach O, d.h. löse das LGS $(O | A)$, sodass die linke Matrix $I_N$ entspricht; die \textbf{rechte Matrix entspricht dann R}
	\end{enumerate}
	\section{Isometrien}
	Seien V, W euklidische (unitäre) Vektorräume und $\{c_1, ..., c_n\}$ eine Orthonormalbasis von V sowie $\phi: V \rightarrow W$ eine lineare Abbildung und $v, w \in V$ beliebig.
	
	\subsection{Isometrien}
	\textbf{Definition}: Längenerhaltende Abbildung zwischen metrischen Räumen; folgende Aussagen zu Isometrien sind äquivalent:
	\begin{itemize}
		\item $\phi$ ist eine Isometrie
		\item $\langle v, w\rangle = \langle \phi(v), \phi(w)\rangle$
		\item $\langle \phi(e), phi(e)\rangle = 1$ für alle Einheitsvektoren $e \in V$
		\item $\{\phi(c_1), \phi(c_2), ..., \phi(c_n)\}$ ist eine Orthonormalbasis von W
		\item Falls V = W: Die Abbildungsmatrix $D_{CC}(\phi)$ ist orthogonal (unitär)
	\end{itemize}
	\textbf{Weiter gilt für alle Isometrien:}
	\begin{itemize}
		\item Jeder Eigenwert von $\phi$ hat Betrag 1
		\item $|det \phi| = 1$
		\item In einem n-dimensionalen Raum lässt sich jede lineare Isometrie als Verknüpfung von maximal n Spiegelungen schreiben
	\end{itemize}

	\subsection{Isometrie-Normalform}
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Aufstellen des char. Polynoms, Berechnung der Eigenwerte
		\begin{itemize}
			\item Ist die Matrix unitär oder euklidisch und diagonalisierbar, zerfällt das char. Polynom in Linearfaktoren, die Diagonalmatrix ist die Normalform und wir sind fertig
		\end{itemize}
		\item Ansonsten: Aufstellen einer Matrix mit Eigenwerten oder Drehkästchen auf der Diagonalen
		\begin{itemize}
			\item Char. Polynom zerfällt in bekannte \textbf{Linearfaktoren}, für welche nur der Eigenwert auf der Hauptdiagonalen eingetragen wird (z.B $(X + 1)$) und kompliziertere Faktoren (z.B $(X^2 - \frac{49}{25}X + 1)$), welche \textbf{Drehkästchen} ergeben
			\item \textbf{Bestimmung von Drehkästchen}: Lösen der quadr. Gleichung über Lösungsformel, Ergebnis hat komplexe Form (hier: $\frac{49}{50} + \frac{3}{50}\sqrt{11}i$), Definition dieser Form als
			\begin{center}
				$cos(\omega) + isin(\omega)$
			\end{center}
			und eintragen als Drehkästchen D mit:
			\begin{center}
				$D = \begin{bmatrix}
					cos(\omega) & -sin(\omega)\\
					sin(\omega) & cos(\omega)\\
				\end{bmatrix}
				\overrightarrow{Normalform} \begin{bmatrix}
					-1 & 0 & 0 \\
					0 & cos(\omega) & -sin(\omega)\\
					0 & sin(\omega) & cos(\omega)\\
				\end{bmatrix}$
			\end{center}
		\end{itemize}

	\end{enumerate}
	
	\section{Adjungierte und normale Abbildungen}
	Sei $K \in \{\realnumbers, \complexnumbers\}$, V, W seien K-Vektorräume sowie $\phi: V \rightarrow W$ eine lineare Abbildung. Die Vektorräume haben Skalarprodukte $\langle \cdot, \cdot\rangle_V$ und $\langle \cdot, \cdot\rangle_W$ sowie Orthonormalbasen $C_V, C_W$.
	
	\subsection{Adjungierte Abbildungen}
	Eine Abbildung $\phi^*: W \rightarrow V$ heißt \textbf{adjungiert} zu $\phi$, falls für alle $v, w \in V$ gilt:
	\begin{center}
		$\langle \phi(v), w\rangle = \langle v, \phi^*(w)\rangle$
	\end{center}
	\textbf{Eigenschaften}:
	\begin{itemize}
		\item Existiert eine zu $\phi$ adjungierte Abbildung, so ist diese \textbf{eindeutig}
		\item Sind V und W endlichdimensional, so existiert $\phi^*$
		\item Es gilt: $D_{C_VC_W}(\phi^*) = \overline{D_{C_VC_W}(\phi)}^T$ sowie $(\phi^*)^* = \phi$
		\item Für einen weiteren K-Vektorraum Z mit Skalarprodukt und einer linearen Abbildung\\ $\psi: W \rightarrow Z$ mit adjungierter Abbildung $\phi^*$ gilt:
		\begin{center}
			$(\psi \circ \phi)^* = \phi^* \circ \psi^*$
		\end{center}
		\item Für jeden Eigenwert $\lambda$ von $\phi$ ist $\overline{\lambda}$ ein Eigenwert von $\phi^*$
	\end{itemize}
	\textbf{Berechnung der adjungierten Abbildung}:\\
	Mit einer Orthonormalbasis $C_V = \{c_1, c_2, ..., c_n\}$ von V gilt:
	\begin{center}
		$v = \sum_{j = 1}^{n}\langle v, c_i\rangle c_i \Rightarrow \phi^*(w) = \sum_{i = 1}^{n}\langle \phi^*(w), c_i\rangle c_i = \sum_{i = 1}^{n}\langle w, \phi(c_i)\rangle c_i$
	\end{center}
	
	\subsection{Selbstadjungierte Homomorphismen}
	Selbstadjungierte Homomorphismen sind ein Spezialfall von adjungierten Abbildungen. Sei V ein Vektorraum mit Skalarprodukt, dann ist die Abbildung $\phi: V \rightarrow V$ \textbf{selbstadjungiert}, wenn gilt:
	\begin{center}
		$\langle \phi(v), w\rangle = \langle v, \phi(w)\rangle\ \forall v\in V$
	\end{center}
	\textbf{Eigenschaften, wenn eine selbstadjungierte Abbildung existiert}:
	\begin{itemize}
		\item Alle Eigenwerte von $\phi$ sind reell
		\item Das char. Polynom von $\phi$ zerfällt in reelle Linearfaktoren
	\end{itemize}
	$\phi$ ist genau dann selbstadjungiert, wenn $\phi$ \textbf{nur reelle Eigenwerte} und eine \textbf{Orthonormalbasis aus Eigenvektoren} besitzt.

	\subsection{Normale Homomorphismen}
	Sei V ein Vektorraum mit Skalarprodukt und $\phi: V \rightarrow V$ eine lineare Abbildung. $\phi$ heißt \textbf{normal}, falls $\phi^*$ existiert und eine der folgenden Bedingungen erfüllt ist:
	\begin{itemize}
		\item $\phi \circ \phi^* = \phi^* \circ \phi$
		\item $\forall v,w \in V: \langle \phi(v), \phi(w)\rangle = \langle \phi^*(v), \phi^*(w)\rangle$
	\end{itemize}
	Mit einer \textbf{Abbildungsmatrix} $A = D_{CC}(\phi)$ bzgl. einer \textbf{Orthonormalbasis} C von V gilt auch:
	\begin{center}
		$\phi$ ist normal $\Leftrightarrow A \overline{A}^T = \overline{A}^TA$
	\end{center}
\end{document}