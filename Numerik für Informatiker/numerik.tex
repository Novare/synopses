\documentclass[10pt,a4paper]{article}
\author{Jannik Koch}
\title{Numerik für Informatiker}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{mathrsfs}
\usepackage{csquotes}
\usepackage{hyperref}

\def\realnumbers{{\rm I\!R}}
\def\polynomials{{\rm I\!P}}

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\renewcommand{\arraystretch}{1.5}
\newcommand{\quotestyle}[1]{\enquote{#1}}

\begin{document}
	\pagenumbering{Roman}
	{\let\newpage\relax\maketitle}
	\tableofcontents
	\newpage
	\pagenumbering{arabic}
	\setcounter{page}{1}

	\section{Grundlagen}
	Falls nicht anders genannt gilt für folgende Abschnitte: $A \in \realnumbers^{N \times N}, b \in \realnumbers^N$.
	
	\subsection{Vorwärts-/Rückwärtssubstitution}
	Zugrunde liegendes Problem ist die Lösung des linearen Gleichungssystems: $Ax = b$.\\\\
	\textbf{Voraussetzung:}\\A sei eine rechte obere oder linke untere Dreiecksmatrix.\\\\
	\textbf{Vorgehen:}
	\begin{enumerate}
		\item Lese die Lösung an der Stelle der Dreiecksmatrix ab, wo die Zeile nur einen Wert $\neq 0$ enthält
		\item Nutze diese Lösung in der nächsten Zeile um die nächste eindeutige Lösung zu ermitteln
		\item Wiederhole dies, bis alle Lösungen gefunden sind
		\item \textbf{Vorwärtssubstitution}:\\Matrix ist eine untere linke Dreiecksmatrix (Lösen von oben nach unten)
		\item \textbf{Rückwärtssubstitution}:\\Matrix ist eine obere rechte Dreiecksmatrix (Lösen von unten nach oben)
		\item Aufwand für Vor- bzw. Rückwärtssubstitution: ca. $\frac{N^2}{2}$
	\end{enumerate}
	\textbf{Bsp.:} Rückwärtssubstitution
	\begin{center}
		$\begin{bmatrix} 1 & 1\\0 & 3\\\end{bmatrix}x = \begin{bmatrix}2 \\ 3\end{bmatrix} \Rightarrow
		x_2 = 1 \Rightarrow x_1 + 1 = 2 \Rightarrow x_1 = 1 \Rightarrow x = \begin{bmatrix}1 \\ 1\end{bmatrix}$
	\end{center}
	
	\subsection{Gleitkommazahlen}
	\subsubsection{Darstellung}
	\begin{itemize}
		\item Darstellung einer Gleitkommazahl z: $z = a * d^e$
		\item $d$: \textbf{Basis}, im Zweiersystem eine Zweierpotenz (2, 4, 8 etc.)
		\item $e$: \textbf{Exponent}, eine ganze Zahl zwischen $e_{min}$ und $e_{max}$
		\item $a$: \textbf{Mantisse}, entweder 0 oder eine Zahl mit $d^{-1} \leq |a| < 1$ der Form $a = v \sum_{i = 1}^{l} a_id^{-i}$ mit dem Vorzeichenbit v und der Mantissenlänge l
		\item \textbf{Relative Maschinengenauigkeit}: $eps = \frac{d^{(1-l)}}{2}$
		\item \textbf{Rundungsfunktion} $rd(x)$ rundet auf ein maschinell darstellbares Format
		\item \textbf{Normierung}:
		\begin{itemize}
			\item Eine Gleitkommazahl ist normiert, wenn der \textbf{Ganzzahlenteil der Mantisse} exakt $1$ ist
			\item Normierung wird durch \textbf{Verschieben der Mantisse} und entsprechendes \textbf{Erhöhen/Vermindern des Exponents} erreicht
			\item \textbf{Bsp}.: $1101.01 * 2^0 = 1.10101 * 2^3$
		\end{itemize}
	\end{itemize}
	\newpage
	\subsubsection{Operationen}
	\begin{itemize}
		\item Standard-Operationen verfügbar, Rundung nach Ausführung $\Rightarrow$ Fehlerquelle!
		\item Aufgrund Rundung sind Operationen nicht assoziativ
		\item \textbf{Auslöschung}: Großer relativer Fehler bei der Subtraktion fast gleich großer Gleitkommazahlen
		\begin{itemize}
			\item Bsp.: Betrachte die untere Gleichung für ein $x \gg 1$: $\sqrt{x + \frac{1}{x}} - \sqrt{x - \frac{1}{x}}$
			\item \textbf{Ist x groß genug, so ist das Ergebnis mathematisch gesehen 0}
			\item Da aber bei der Subtraktion beider Wurzeln zwei ca. gleich große Zahlen subtrahiert werden, entsteht ein \textbf{großer relativer Fehler!}
			\item \textbf{Umstellen der Gleichung schafft Abhilfe durch Entfernen der Subtraktion}:
			\begin{center}
				$\sqrt{x + \frac{1}{x}} - \sqrt{x - \frac{1}{x}}
				= \frac{(\sqrt{x + \frac{1}{x}} - \sqrt{x - \frac{1}{x}})(\sqrt{x + \frac{1}{x}} + \sqrt{x - \frac{1}{x}})}{(\sqrt{x + \frac{1}{x}} + \sqrt{x - \frac{1}{x}})}
				= \frac{\frac{2}{x}}{(\sqrt{x + \frac{1}{x}} + \sqrt{x - \frac{1}{x}})}$
			\end{center}
		\end{itemize}
		\item Es gilt: $1 + |y| = 1,\ falls\ |y| < eps$
	\end{itemize}

	\subsection{Matrixnormen}
	\begin{enumerate}
		\item{\makebox[13cm]{\textbf{Spaltensummennorm}: Summiere Spalten, wähle Maximalwert\hfill} $\norm{A}_1$}
		\item{\makebox[13cm]{\textbf{Spektralnorm}: Wurzel des größten Eigenwerts von $A^TA$\hfill} $\norm{A}_2$}
		\item{\makebox[13cm]{\textbf{Zeilensummennorm}: Summiere Zeilen, wähle Maximalwert\hfill} $\norm{A}_\infty$}
	\end{enumerate}
	
	\subsection{Konditionen}
	\textbf{Konditionszahl}:
	Sei $f: \realnumbers^N \rightarrow \realnumbers^K$ eine differenzierbare Funktion und $x \in \realnumbers^N$
	\begin{itemize}
		\item Maß für den Einfluss der Störungen von A und b auf x (wie sensibel ist das LGS?)
		\item Kondition einer Matrix: $1 \leq cond(A) := \norm{A}\norm{A^{-1}}$, wobei $cond(A) = cond(\alpha A), \alpha\in\realnumbers\setminus\{O\}$
		\item Absolute Konditionszahl: $\kappa^{kn}_{abs}(x) = |\frac{\delta}{\delta x_n}f_k(x)|$
		\item Relative Konditionszahl: $\kappa^{kn}_{rel}(x) = |\frac{\delta}{\delta x_n}f_k(x)| \frac{|x_n|}{|f_k(x)|}$ falls $f_k(x) \neq 0$
	\end{itemize}
        Für die \textbf{Kondition} $\kappa$ eines Problems gilt die \textbf{Bewertung}:
        \begin{itemize}
                \item Für $\kappa \approx 1$ gut konditioniertes Problem
                \item Für $\kappa > 1$ schlecht konditioniertes Problem
                \item Für $\kappa \rightarrow \infty$ schlecht gestelltes Problem
        \end{itemize}

    \newpage
	\subsection{Givens-Rotationen}
	\begin{itemize}
		\item \textbf{Geometrisch}: Drehung in einer Ebene, die durch zwei Koordinaten-Achsen aufgespannt wird
		\item Nützlich für \textbf{Eigenwertberechnung} und \textbf{QR-Zerlegung} (s. unten)
		\item Darstellbar als \textbf{orthogonale} Matrix über $\realnumbers$; sei hierfür $\phi$ der Rotationswinkel, $s = sin \phi$ sowie $ c = cos \phi$:
		\begin{center}
			$\begin{bmatrix}
				1 		& \dots 	& 0 	 & \dots 	& 0 	 & \dots 	& 0 	 \\
				\vdots 	& \ddots 	& \vdots & 		 	& \vdots &  	 	& \vdots \\
				0 		& \dots 	& c 	 & \dots 	& s 	 & \dots 	& 0 	 \\
				\vdots 	&			& \vdots & \ddots 	& \vdots &  	 	& \vdots \\
				0 		& \dots 	& -s 	 & \dots 	& c 	 & \dots 	& 0 	 \\
				\vdots 	&			& \vdots & 		 	& \vdots & \ddots 	& \vdots \\
				0 		& \dots 	& 0 	 & \dots 	& 0 	 & \dots 	& 1 	 \\
			\end{bmatrix}$
		\end{center}
	\end{itemize}

    \subsection{Householder-Spiegelungen}
    \begin{itemize}
    	\item \textbf{Geometrisch}: Spiegelung eines Vektors an einer Hyperebene durch Null im euklidischen Raum
    	\item \textbf{Hyperebene}: \quotestyle{Ebene} beliebiger Dimension
		\item Nützlich für \textbf{QR-Zerlegung} (s. unten)
    	\item \textbf{Euklidischer Raum}: Über $\realnumbers$ definierter Vektorraum mit Skalarprodukt
    	\item Darstellbar als \textbf{symmetrische, orthogonale} Matrix $Q$ über $\realnumbers$, wobei $v$ der zur \textbf{Hyperebene gehörende Normalenvektor} ist:
    	\begin{center}
    		$Q = I - \frac{2}{v^Tv}vv^T$
    	\end{center}
    \end{itemize}

	\newpage
	\section{Zerlegungen}
	Falls nicht anders genannt gilt für folgende Abschnitte: $A, R, L, Q \in \realnumbers^{N \times N}, b \in \realnumbers^N$.

	\subsection{LR-Zerlegung}
	\textbf{Ziel}:\\Zerlegung von A in eine \textbf{rechte obere Dreiecksmatrix R} und eine \textbf{linke untere\\Dreiecksmatrix L}, sodass gilt: $Ax = LRx = b$
	\subsubsection{Berechnung}
	\begin{enumerate}
		\item Schreibe Matrix als Produkt $I_N * A$ mit der Einheitsmatrix $I_N$
		\item Forme schrittweise die rechte Matrix zu R um und notiere die Änderungen in der linken Matrix folgendermaßen:
		\begin{enumerate}
			\item Jede Operation wird als $Zeile\ A - Faktor * Zeile\ B$ notiert, auch\\
			Additionen (z.B als \rom{2}$ - (-2)$\rom{1}); Zeilen vertauschen ist nicht gestattet!
			\item Notiere den Vorfaktor an der Stelle, an der in R eine 0 entstanden ist
		\end{enumerate}
	\end{enumerate}
	\textbf{Bsp.:} $2\times2$ LR-Zerlegung
	\begin{center}
		$A = 
		\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} =
		\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} * \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} \stackrel{II - 3I}{=}
		\begin{bmatrix}1 & 0 \\ 3 & 1\end{bmatrix} * \begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix} = LR
		$	
	\end{center}
	\subsubsection{Berechnung mit Pivot-Suche}
	\textbf{Idee:}\\A lässt sich durch Zeilenvertauschungen immer in eine Form bringen, sodass eine LR-Zerlegung sicher existiert (falls A regulär). Hiermit lässt sich auch die Konditionierung der Zerlegung verbessern.\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Wähle eine Permutationsmatrix P und zerlege PA, sodass gilt: $PA = LR$
		\item Löse $LRx = Pbx$
	\end{enumerate}
	\textbf{Permutationsmatrix für bessere Konditionierung durch Spaltenpivotsuche}:
	\begin{enumerate}
		\item Iteriere über die n Zeilen der Matrix A
		\item Bestimmte k, sodass $|A[k, n]| \geq |A[j, n]|$ für $j = n, ..., N$, d.h. suche in der aktuellen Spalte nach einem Eintrag unterhalb des aktuellen Eintrags, der betragsmäßig größer oder gleich ist
		\item Vertausche die Zeilen n und k, falls $k \neq n$
	\end{enumerate}
	\textbf{Performantes Lösen einer fertigen LR-Zerlegung:}
	\begin{enumerate}
		\item Substituiere $Rx$ mit $y\in\realnumbers^N$ und löse $Ly = b$ per Vorwärtssubstitution
		\item Löse $Rx = y$ per Rückwärtssubstitution
	\end{enumerate}
	\subsection{Cholesky-Zerlegung}
	\textbf{Ziel}:\\Zerlegung von A in eine \textbf{reguläre linke untere Dreiecksmatrix L}, wobei \textbf{A symmetrisch und positiv definit sein muss}, sodass gilt: $A = LL^T$\\\\
	\textbf{Berechnen der Cholesky-Zerlegung:}
	\begin{itemize}
		\item Alle Einträge über der Diagonalen sind 0, für alle anderen Einträge befolge dies zeilenweise:
		\begin{enumerate}
			\item Berechne die Summe $s_{ij} = a_{ij} - \sum_{k = 1}^{j - 1} l_{ik}l_{jk}$
			\item Für einen Diagonaleintrag gilt: $l_{ii} = \sqrt{s_{ii}}$ 
			\item Für einen Eintrag unter der Diagonalen gilt: $l_{ij} = \frac{s_{ij}}{l_{jj}}$
		\end{enumerate}
	\end{itemize}
	\textbf{Bsp.:} 3x3 Cholesky-Zerlegung
	\begin{center}
		$A 	= \begin{bmatrix}
				4 & 2 & 2\\
				2 & 17 & 5\\
				2 & 5 & 11
			\end{bmatrix}$\\
			\vspace*{0.5cm}
		$L 	= \begin{bmatrix}
				l_{11} & 0 & 0\\
				l_{21} & l_{22} & 0\\
				l_{31} & l_{32} & l_{33}
			\end{bmatrix}
			= \begin{bmatrix}
		  		\sqrt{4 - 0} & 0 & 0\\
		  		\frac{2 - 0}{2} & \sqrt{17 - 1 * 1} & 0\\
		  		\frac{2 - 0}{2} & \frac{5 - 1 * 1}{4} & \sqrt{11 - (1 * 1 + 1 * 1)}
 			\end{bmatrix}
 			= \begin{bmatrix}
	 			2 & 0 & 0\\
	 			1 & 4 & 0\\
	 			1 & 1 & 3
 			\end{bmatrix}$
	\end{center}
	\subsection{QR-Zerlegung}
	\textbf{Ziel}:\\Zerlegung von A in eine \textbf{rechte obere Dreiecksmatrix R} und eine \textbf{orthogonale Matrix Q}, sodass gilt: $A = QR$. Dies ist nützlich, wenn A nicht quadratisch ist, da die LR-Zerlegung dann nicht funktioniert.\\\\
	\textbf{Berechnen der QR-Zerlegung}:
	\begin{enumerate}
		\item Berechne $\alpha = sgn(a_{11})\norm{a_1}$ und damit $v_1 = a_1 + \alpha e_1$ und damit $Q_1 = I - \frac{2vv^T}{v^Tv}$ (Householder-Spiegelung)
		\item Berechne $Q_1A$. Wähle von diesem Ergebnis eine Untermatrix (ohne erste Zeile und erste Spalte) und wiederhole das Verfahren mit dieser und erhalte $Q_2^*$.
		\item Bette $Q_2^*$ in eine Einheitsmatrix mit denselben Dimensionen wie A ein und erhalte $Q_2$.
		\item Wiederhole 2. und 3. bis $Q_n * Q_{n - 1} * ... Q_1A = R$ Dreiecksgestalt hat.
		\item Multipliziere von links $Q_1^T * Q_2^T * ... Q_n^T = Q$ an beide Seiten.
	\end{enumerate}
	\newpage
	\noindent\textbf{Performantes Lösen einer fertigen QR-Zerlegung}:
	\begin{enumerate}
		\item A sei überbestimmt (keine LR-Zerlegung möglich) und $Ax = QRx = b$
		\item Multipliziere $A^T$ auf beiden Seiten: $A^TAx = (QR)^TQRx = R^TQ^TQRx = R^TRx = b$
		\item Löse $R^TRx = b$ analog zum LR-Verfahren per Vorwärts- und Rückwärtssubstitution
	\end{enumerate}
	\subsection{Aufwand}
	\begin{itemize}
		\item QR-Zerlegung mit N = M:\hfill$\frac{4}{3}N^3$ Operationen
		\item LR-Zerlegung:\hfill$\frac{2}{3}N^3$ Operationen
		\item Cholesky-Zerlegung:\hfill$\frac{1}{3}N^3$ Operationen
	\end{itemize}
	\newpage
	\section{Lineare Ausgleichsrechnung}
	\textbf{Problem}:\\Lineare Gleichungssysteme oft nicht oder nicht eindeutig lösbar.\\\\
	\textbf{Lösung}:\\Versuche die bestmögliche Lösung zu finden, sodass gilt: für $x \in \realnumbers^N$ ist $|Ax - b|_2$ minimal

	\subsection{Per QR-Zerlegung}
	\textbf{Voraussetzung}:\\$A^TA$ ist invertierbar und gut konditioniert\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item Berechne QR-Zerlegung: $A = QR$, dann ist $A^TA = R^TR$ (s. Kapitel zu QR-Zerlegung)
		\item Löse $Rx = Q^Tb$
	\end{enumerate}
	
	\subsection{Per Singulärwertzerlegung}
	\textbf{Idee}:\\\textbf{Alternative zum QR-Ansatz} falls gilt: $A^TA$ ist \textbf{singulär} (besitzt keine Inverse) oder \textbf{schlecht konditioniert}. Sei für diesen Fall $A \in \realnumbers^{K\times N}$. Zerlege A folgendermaßen:
	\begin{align*}
		R &= rang(A)\\
		V &= (v_1 | ... | v_R) \in \realnumbers^{K \times R}& (V^TV = I_R)\\
		U &= (u_1 | ... | u_R) \in \realnumbers^{N \times R}& (U^TU = I_R)\\
		\Sigma &= diag(\sigma_1, ..., \sigma_R) \in \realnumbers^{R\times R}& (\sigma_r > 0, r = 1,...,R)\\
		A &= V\Sigma U^T = \sum^R_{r=1}\sigma_rv_ru_r^T &
	\end{align*}

	\subsection{Normalengleichung}
	\begin{itemize}
		\item \textbf{Pseudo-Inverse}: Die Pseudo-Inverse zu $A$ ist $A^+ = U\Sigma^{-1}V^T = \Sigma^R_{r = 1}\frac{1}{\sigma_r}u_rv_r^T$
		\item \textbf{Normalengleichung}: $x$ minimiert den Term $|Ax - b|_2$ genau dann, wenn $x$ die\\Normalengleichung löst: $$A^TAx = A^Tb \Leftrightarrow x = A^+b$$
	\end{itemize}	
	
	\subsection{(Tikhonov)-Regularisierung}
	\textbf{Problem}:\\Eine Aufgabe ist potenziell schlecht konditioniert (sorgt für extreme Datenfehler) oder nicht sachgemäß gestellt (und damit nicht eindeutig lösbar oder nicht stetig abhängig von den Daten).\\\\
	\textbf{Lösung}:\\Regularisiere die Aufgabe, sodass sie gut konditioniert bzw. sachgemäß gestellt ist. Hierzu wird die sog. Tikhonov-Regularisierung ($F_\alpha(x)$) minimiert.
	
	\newpage
	\section{Eigenwertberechnung}
	\textbf{Ziel:}\\Berechnung von Eigenwerten und Eigenvektoren (z.B für Zwecke der Informatik wie das Lösen des Problems \quotestyle{Welche Seite hat einen Link auf welche Seite?} oder der Technik für Wellengleichungen).

	\subsection{Hessenberg-Matrizen}
	\textbf{Relevanz}:\\Hessenberg-Matrizen sind aufgrund ihrer Form ein wichtiger Teilschritt im unten folgenden QR-Algorithmus für Eigenwertprobleme\\\\
	\textbf{Form}:\\Quadratische Matrizen, deren Einträge unter- bzw. überhalb der ersten Nebendiagonalen gleich $0$ sind\\\\
	\textbf{Hessenberg-Transformation}:\\Berechnung einer Matrix $Q$ für $A \in \realnumbers^{N\times N}$ in $O(N^3)$, sodass $H = QAQ^T$:
	\begin{enumerate}
		\item Berechne $v = \begin{pmatrix}0\\A[2:N,1]\end{pmatrix} \in \realnumbers^N$, $\sigma = -sgn(v[2])|v|$ und $w = \frac{1}{v[2] - \sigma}(v - \sigma e^2)$
		\item Berechne damit die Householder-Spiegelung: $Q_1 = I_N - \frac{2}{w^Tw}ww^T$
		\item Berechne damit $A_1 = Q_1AQ_1$ und fahre mit $A_1[2:N, 2:N]$ sukzessive fort bis die gewünschte Form erreicht ist
	\end{enumerate}

	\subsection{Inverse Iteration mit variablem Shift}
	\textbf{Rayleigh-Quotient}:\\Definiere $r(A, v) = \frac{v^TAv}{v^Tv}$ für $A \in \realnumbers^{N\times N}, v \in \realnumbers^N, v \neq 0$\\\\
	\textbf{Idee}:\\Wähle einen \textbf{Startvektor} $0 \neq z^0 \in \realnumbers^N$ und nähere diesen \textbf{iterativ} an einen Eigenvektor von $A$ an, bis der Unterschied zu einem echten Eigenvektor $\epsilon \geq 0$ erreicht; die \textbf{Laufvariable} sei $k = 0$\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item \textbf{Normierung und Rayleigh-Quotient}:\\Berechne $w^k = \frac{1}{|z^k|_2}z^k$ sowie $\mu_k = r(A, w^k)$ (\quotestyle{Shift})
		\item \textbf{Betrachte Nähe zum Eigenvektor}:\\Falls $|Aw^k - \mu_k w^k|_2 \leq \epsilon$ sind wir fertig, ansonsten fahre fort
		\item \textbf{Annäherungsschritt}:\\$z^{k+1} = (A - \mu_k * I_N)^{-1}w^k$, erhöhe $k$ um $1$ und wiederhole
	\end{enumerate}

	\newpage
	\subsection{QR-Iteration mit variablem Shift}
	\textbf{Idee}:\\Iterative Annäherung der Eigenvektoren einer symmetrischen Matrix $A \in \realnumbers^{N\times N}$ per \textbf{QR-Zerlegung} bis ein $\epsilon \geq 0$ erreicht wird; die \textbf{Laufvariable} sei $k = 0$. Die Annäherung konvergiert gegen eine\\\textbf{Diagonalmatrix}, aus der die Eigenwerte dann einfach ablesbar sind.\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item \textbf{Iterationsbremse}:\\Falls $|A_k[n, n-1]| \leq \epsilon$, dekrementiere $n$; falls dann $n = 1$, sind wir fertig
		\item \textbf{QR-Zerlegung}:\\Wähle $\mu_k = A[n,n]$ und berechne die \textbf{QR-Zerlegung}: $Q_kR_k = A_k - \mu_kI_N$
		\item \textbf{Neue Iteration}:\\Setze $A_{k+1} = R_kQ_k + \mu_kI_N$, inkrementiere $k$ und wiederhole
	\end{enumerate}
	\textbf{Eigenwerte}:\\Die Eigenwerte bleiben bei jeder Iteration gleich, da $A_{k + 1} = Q_k^TA_kQ_k$

	\subsection{QR-Iteration mit variablem Shift und Hessenberg-Matrix}
	\textbf{Idee}:\\QR-Iteration, aber wähle statt $A$ die \textbf{Hessenberg-Transfomierte} als Startmatrix,\\d.h. $A_0 = QAQ^T$\\\\
	\textbf{Vorgehen}:
	\begin{enumerate}
		\item \textbf{Iterationsbremse}:\\
		Falls $|A_k[n + 1, n]| \leq \epsilon$ für ein $n$, führe eine \textbf{getrennte Eigenwertberechnung} für\\$A_k[1 : n, 1 : n]$ und $A_k[n + 1 : N, n + 1 : N]$ aus
		\item \textbf{QR-Zerlegung}:\\
		Berechne Shift $\mu_k$ (genaue Berechnung nicht relevant) und damit die \textbf{QR-Zerlegung} $Q_kR_k = A_k - \mu_kI_N$
		\item \textbf{Neue Iteration}:\\
		Setze $A_{k+1} = R_kQ_k + \mu_kI_N$, inkrementiere $k$ und wiederhole
	\end{enumerate}
	\textbf{Hessenberg-Matrix}:\\Durch die Transformation zu einer Hessenberg-Matrix steigt der \textbf{Aufwand} zu $O(N^3)$, jedoch lässt sich hierdurch die Berechnung in separate \textbf{Unterprobleme} teilen (vgl. \textbf{Iterationsbremse})\\\\
	\textbf{Aufwand eines Schrittes}:\\Jeder Schritt benötigt $O(N)$ Operationen

	\newpage
	\section{Iterationsverfahren für lineare Gleichungssysteme}
	\textbf{Ziel}:\\Iteratives Lösen eines linearen Gleichungssystems.\\\\
	\textbf{Anwendung}:\\Graphentheorie z.B zur Bisektion eines Graphen, Berechnung der Auslenkung einer Membran unter einer bestimmten Last in der Technik.
	\subsection{Jacobi-Verfahren (Gesamtschrittverfahren)}
	\textbf{Idee}:\\Approximiere die Lösung $x \in \realnumbers^N$. Wähle x dazu beliebig und setze einen Schwellenwert $\epsilon > 0$ sowie $k = 0$. Solange $|Ax - b|_2 > \epsilon$, berechne
	\begin{center}
		$x^{k+1}_m = (b_m - \sum_{n \neq m}^{} A[m, n]x_n^k) / A[m, m]$ für $m = 1, ..., N$
	\end{center}
	und erhöhe k um 1. Erweitert man diesen Algorithmus mit der Idee, dass die $x_n$ bei der Berechnung von $x_m$ schon bekannt sind, falls $n < m$, so erhält man das Einzelschrittverfahren.
	\subsection{Gauß-Seidel-Verfahren (Einzelschrittverfahren)}
	Vergleiche Jacobi-Verfahren und ändere die Formel für die nächstbeste Approximation von x zu:
	\begin{center}
		$x^{k+1}_m = (b_m - \sum_{n = 1}^{m - 1}A[m, n]x^{k+1}_n - \sum_{k = m + 1}^{N}A[m, n]x_n^k) / A[m, m]$ für $m = 1, ..., N$
	\end{center}
	
	\subsection{Iterationsmatrizen}
	Iterationsverfahren können für Beweise bzgl. des Konvergenzverhaltens in Matrixform geschrieben werden. Hierzu zerteilen wir A in eine \textbf{strikte untere linke Dreiecksmatrix} $L$, eine \textbf{strikte obere rechte Dreiecksmatrix} $R$ und eine \textbf{Diagonalmatrix} $D$ mit $$A = L + D + R$$
	Die \textbf{Iterationsmatrizen} für die einzelnen Verfahren werden auch als \textbf{Vorkonditionierer} bezeichnet und entsprechen im \textbf{Jacobi-Verfahren} $D^{-1}$ und im \textbf{Gauß-Seidel-Verfahren} $(D + L)^{-1}$.\\\\

	\noindent\textbf{Für das Jacobi-Verfahren gilt damit}:
	$$x^{k+1} = x^k + D^{-1}(b - Ax^k)$$

	\noindent\textbf{Für das Gauß-Seidel-Verfahren gilt damit}:
	$$x^{k+1} = x^k + (D + L)^{-1}(b - Ax^k)$$

	\subsection{Konvergenz}
	Das Iterationsverfahren konvergiert genau dann für jeden Startvektor $x_0$, falls der Spektralradius (betragsmäßig größter Eigenwert) der Iterationsmatrix kleiner als 1 ist.

	\newpage
	\section{Iterationsverfahren für nichtlineare Gleichungssysteme}
	\subsection{Newton-Verfahren}
	\textbf{Idee}:\\Zugrunde liegende Gleichung ist \textbf{nichtlinear} (z.B Polynom von mindestens Grad zwei, Bruchgleichungen, Wurzelgleichungen, Exponentialgleichungen etc.). Nähere hierfür die Lösung durch die \textbf{Newton-Iteration} an. Hierbei bestimmt man die \textbf{Tangente des aktuellen Punkts}, wählt die \textbf{Nullstelle der Tangente} und fährt von dieser an fort. Für einen Vektor $x_k$ folgt für eine Funktion $F(x)$ die nächste Näherung durch die Newton-Iteration $x_{k + 1} = x_k - \frac{F(x_k)}{F'(x_k)}$.\\\\
	\textbf{Das Newton-Verfahren konvergiert nur für einen guten Startwert!}\\\\
	\textbf{Lösungssuche über Newton-Verfahren}:\\
	Bsp.: Finden der n-ten Wurzel
	\begin{enumerate}
		\item Gesucht: $x = \sqrt[n]{2}$\\formuliere als Nullstellenproblem: $x^n = 2 \Leftrightarrow x^n - 2 = 0 \Rightarrow F(x) = x^n - 2$ und $F'(x) = nx^{n - 1}$
		\item Iteriere für z.B $n = 5$ folgendermaßen:
		\begin{enumerate}
			\item $x_1 = 2$
			\item $x_2 = 2 - \frac{2^5 - 2}{5 * 2^4} = 2 - \frac{30}{80} = \frac{13}{8}$
			\item $x_3 = \frac{13}{8} - \frac{(\frac{13}{8})^5 - 2}{5 * (\frac{13}{8})^4} \approx 1.357364938$
			\item etc. (Lösung des Taschenrechners: $x \approx 1.148698355$)
		\end{enumerate}
	\end{enumerate}
	\textbf{Lösen einer Minimierungsaufgabe}:                                                                                        \\
	Zur Minimierung einer differenzierbaren Funktion $f: \realnumbers^N \rightarrow \realnumbers$ suche $x^* \in \realnumbers^N$ mit \\$f(x^*) \leq f(x) \forall x \in \realnumbers^N$. Dies lässt sich erreichen, indem man das Nullstellenproblem\\$F(x) = grad\ f(x) = 0$ berechnet. Die damit folgende Newton-Iterationsvorschrift lautet:
	\begin{center}
		$x_{k + 1} = x_k - F''(x_k)^{-1} grad\ f(x_k)$
        \end{center}
        \textbf{Allgemeine Gleichung für mehrdimensionale Probleme}
        \begin{enumerate}
        	\item Wähle Startwert $x_0$ und $\epsilon$
        	\item Falls $|F(x^k)| < \epsilon$: STOP
        	\item Berechne Newton Korrektur $d^k: F^{\prime}(x^k)d^k = -F(x^k)$
        	\item Setze: $x^{k+1} = x^{k} + d^{k}$
        \end{enumerate}

    \newpage
    \section{Polynom-Interpolation}

	\textbf{Problem}:\\Nur eine endliche Punktmenge einer Funktion ist gegeben, zur Erschließung der Kurve kann nur zwischen diesen Punkten interpoliert werden
	Ermöglicht z.B \textbf{Datenkompression}, aber verursacht ebenfalls Interpolationsfehler\\\\
	\textbf{Ein Polynom von Grad N-1 benötigt eine Menge von N Punkten!}\\\\
	\textbf{Runge-Phänomen}:\\Der maximale Fehler einer Interpolation \textbf{steigt} mit Höhe des Grades des Interpolationspolynoms

	\subsection{Lagrange-Interpolation}
	\textbf{Interpolationsaufgabe}:\\Zu \textbf{Stützstellen} $\xi_0 < \xi_1 < ... < \xi_N$ und \textbf{Funktionswerten} $f_0, ..., f_N \in \realnumbers$ bestimme das eindeutige Polynom P mit $P(\xi_i) = f_i$ für $i = 0, ..., N$\\\\
	\textbf{Lagrange-Basis}: $$L_n(t) = \prod^{N}_{k = 0, k \neq n} \frac{t - \xi_k}{\xi_n - \xi_k}$$
	\textbf{Konstruktion über Lagrange-Basis}: $$P(t) = \sum^N_{n = 0}f_nL_n(t)$$
	\textbf{Vorteile}:\\Explizite Darstellung der Lösung\\\\
	\textbf{Nachteile}:\\Für große N \textbf{numerisch instabil}, Auswertung des Polynoms in $O(N^2)$, für neuen Wert muss alles \textbf{neu berechnet} werden
	
	\subsection{Newton-Interpolation}
	\textbf{Newton-Basis}:\\Zu den Stützstellen sei die Newton-Basis definiert als: $$\omega_0 \equiv 1,\ \omega_k(t) = (t - \xi_{k-1})\omega_{k-1}(t)$$
	\textbf{Polynom}:\\Polynom durch Interpolation besteht aus Newton-Basis und passenden Koeffizienten: $$p(x) = a_0 + a_1 * \omega_1 + ... + a_n * \omega_n$$
	Berechnung der Koeffizienten erfolgt über \textbf{Dividierten Differenzen}.
	\newpage
	\noindent\textbf{Dividierten Differenzen}:\\
	Seien folgende Funktionswerte an Stützstellen gegeben:
	\begin{center}
		\begin{tabular}{c | c c c}
			n & 0 & 1 & 2\\
			\hline
			x & 1 & $\frac{3}{2}$ & 2\\
			y & -1 & 0 & $\frac{1}{2}$
		\end{tabular}
	\end{center}
	Berechne paarweise nach folgendem Schema:
	\begin{center}
		\begin{tabular}{l l l}
			$y[1] = -1$ 				&																		&\\
										& $y[1, \frac{3}{2}] = \frac{0 - (-1)}{\frac{3}{2} - 1} = 2$ 			&\\
			$y[\frac{3}{2}] = 0$		& 																		& $y[1, \frac{3}{2}, 2] = \frac{1 - 2}{2 - 1} = -1$\\
										& $y[\frac{3}{2}, 2] = \frac{\frac{1}{2} - 0}{2 - \frac{3}{2}} = 1$		&\\
			$y[2] = \frac{1}{2}$		&																		&
		\end{tabular}
	\end{center}
	Das \textbf{oberste Ergebnis jeder Spalte} ergibt einen \textbf{Koeffizienten}, d.h.: $a_0 = -1, a_1 = 2, a_2 = -1$. Damit:
	$$p(x) = -1 + 2(x - 1) -1(x - 1)(x - \frac{3}{2})$$

	\subsection{Neville-Schema}
	\textbf{Alternativ zum Newton-Schema}:\\Berechnung \textbf{eines Funktionswertes} einer bestimmten Stelle \textbf{statt des kompletten expliziten Polynoms}. \textbf{Rekursionsformel} für den Funktionswert des Interpolationspolynoms an einer bestimmten Stelle mit \textbf{Berücksichtigung der Stützstellen} $i$ bis $j$:
	\begin{center}
		$p[x_i,...,x_j](x) = \frac{(x - x_i) p[x_{i + 1},...,x_j](x) - (x - x_j)p[x_i,...,x_{j-1}](x)}{x_j - x_i}$
	\end{center}
	Rekursives Schema (gesucht ist der Funktionswert an Stelle $x = 4$):\\
	\begin{center}
		\begin{tabular}{c | c l c c}
			$\xi_i$ & $f(\xi_i)$ & & &\\
			\hline
			0 & 1 & &\\
			1 & 2 & $\frac{4 - 0}{1 - 0} * 2 - \frac{4 - 1}{1 - 0} * 1 = 5$ & &\\
			2 & 0 & $\frac{4 - 1}{2 - 1} * 0 - \frac{4 - 2}{2 - 1} * 2 = -4$ & -13 &\\
			3 & 1 & $\frac{4 - 2}{3 - 2} * 1 - \frac{4 - 3}{3 - 2} * 0 = 2$ & 5 & 11 = $P_3(4)$
		\end{tabular}
	\end{center}
	Das Newton-Schema ist aufwendiger, lohnt sich jedoch, wenn viele Stellen ausgewertet werden müssen. Das Neville-Schema ist gut für wenige Suchen geeignet. Für einen weiteren Stützpunkt wird hier \textbf{eine zusätzliche Zeile berechnet, die alten Ergebnisse bleiben valide!}
	\newpage
	\section{Splines}
	\textbf{Idee}:\\Interpoliere nicht mit einem passenden Polynom sondern setze die Lösung mit mehreren Polynomen stückweise zusammen. Diese Funktion wird als Spline bezeichnet (nicht ein einzelnes Stück davon!).\\\\\textbf{Splines sind in der Regel selbst keine Polynome!} Eine einfache Lösung sind \textbf{lineare Splines}, bei denen zwischen den Stützpunkten linear verbunden wird - diese sind jedoch nicht glatt und an den Stützpunkten nicht differenzierbar.
	
	\subsection{Kubische Splines}
	\textbf{Idee}:\\Kubische Polynome, damit der Spline glatt und an Stützpunkten zweifach stetig differenzierbar ist.\\\\
	\textbf{Grundlegende Definitionen}:
	\begin{itemize}
		\item Zu einer Zerlegung $\Xi = \{a = \xi_0 < \xi_1 < ... < \xi_N = b\}$ des Intervalls $[a, b]$ definiere den Raum kubischer Splines:
		\begin{center}
			$\mathscr{S}_3(\Xi) = \{S \in C^2[a, b]: S_n = S|_{[\xi_{n-1}, \xi_n]} \in \polynomials_3, n = 1, ..., N\}$
		\end{center}
		\item Dabei heißt für $f \in C[a, b]$ der Spline $S \in \mathscr{S}_3(\Xi)$ interpolierender kubischer Spline zu f, wenn:
		\begin{center}
			$S(\xi_n) = f(\xi_n)$, $n = 0, ..., N$
		\end{center}
	\end{itemize}
	\textbf{Randbedigungen zur eindeutigen Bestimmung:}\\
	Mit jeweils einer dieser Randbedingungen ist die Spline-Interpolation $S \in \mathscr{S}_3(\Xi)$ zu f eindeutig lösbar:
	\begin{itemize}
		\item Natürliche Randbedingung:\hfill$S''(a) = S''(b) = 0$
		\item Hermite-Randbedingungen zu $f \in C^1[a, b]$:\hfill$S'(a) = f'(a)$ und $S'(b) = f'(b)$
		\item Periodische Randbedingungen:\hfill$S'(a) = S'(b)$ und $S''(a) = S''(b)$
	\end{itemize}
	Weiter seien die \textbf{Momente} $\mu_n = S''(\xi_n)$ eines Interpolationssplines $S \in \mathscr{S}_3(\Xi)$ zu f definiert als $\mu_n = S''(\xi_n)$. Die Momente ergeben sich durch das Lösen von Gleichungssystemen passend zu den Randbedingungen (vgl. Skript, nicht klausurrelevant).\\\\
	\textbf{Berechnung eines Splines durch 3 Punkte}:\\
	\textbf{Gegeben}:\\Spline-Polynom vor und nach dem \textbf{mittleren Stützpunkt} mit variablen Koeffizienten.\\\\
	\textbf{Vorgehen}:\\Berechne für beide Spline-Polynome den \textbf{Funktionswert}, die \textbf{erste Ableitung} und die \textbf{zweite Ableitung} an der mittleren Stützstelle, setze die entsprechenden Ergebnisse gleich und \textbf{löse das LGS für die Koeffizienten}.

	\section{Numerische Integration}
	\textbf{Problem}:\\Eine Stammfunktion für die reguläre Integration kann nicht immer gefunden werden, deshalb wird auf numerische Verfahren zurückgefallen.\\\\
	\textbf{Idee}:\\Approximiere den Wert des Integrals mit dem Wert Q(f) einer Quadraturformel Q.

	\subsection{Definition der Quadratur}
	Für eine endliche Menge Stützstellen $\Xi$ existiert genau eine Quadratur $$Q_\Xi(f) = \sum_{\xi \in \Xi} \omega_\xi f(\xi)$$
	Die \textbf{Kantengewichte} $\omega_\xi$ sind anhand der \textbf{Lagrange-Basis} definiert:
	$$\omega_\xi = \int_a^bL_\xi(t)dt\hspace*{1cm}L_\xi(t) = \prod_{\eta \in \Xi \setminus \{\xi\}} \frac{t - \eta}{\xi - \eta}$$

	\subsection{Fehler einer Quadratur}
	Der \textbf{Fehler einer Quadratur} $Q_\Xi(f)$ ist definiert als $$|Q_\Xi(f) - \int^b_af(t)dt|$$
	Für jede Quadratur die für Polynome $P \in \polynomials_{K-1}$ exakt ist, existiert mit $C > 0$ eine \textbf{obere Schranke des Fehlers} für $f \in C^{K+1}[a,b]$:
	$$|Q_\Xi(f) - \int^b_af(t)dt| \leq C(b - a)^{K+1}\max_{t\in[a,b]}||(\frac{d}{dt})^{K+1}f||$$

	\subsection{Wichtige Quadraturformeln}
        Trapezregel (Ordnung $p = 2$):\hfill$\int_{a}^{b} f(x) = \frac{b - a}{2} (f(a) + f(b))$ \\
        Simpsonregel (Ordnung $p = 4$):\hfill$\int_{a}^{b} f(x) = \frac{b - a}{6} (f(a) + 4f(\frac{a-b}{2}) + f(b))$ \\
        Newton'sche $\frac{3}{8}$-Regel:\hfill$w_0 = w_3 \frac{b - a}{8}, w_1 = w_2 = \frac{3(b - a)}{8}$ \\
        Milne-Regel:\hfill$w_0 = w_4 = \frac{7(b - a)}{90}, w_1 = w_3 = \frac{32(b - a)}{90}, w_2 = \frac{12(b - a)}{90}$ \\

	\subsection{Summierte Trapezregel}
	\textbf{Ansatz (Summierte Quadraturformel)}:\\Zerlege das Intervall $[a, b]$ in gleich große Stücke und wende die Quadraturformel auf jedem Teilinterval an.\\\\
	\textbf{Summierte Trapezregel}:\\Zerlege Flächen unter einer Kurve im gegebenen Intervall in Trapeze.

	\newpage
	\section{Integrationsverfahren für gewöhnliche Differentialgleichungen}
	\subsection{Runge-Kutta-Verfahren}
	\textbf{Ziel}: Numerisches Lösen von Anfangswertproblemen. Definition eines Anfangswertproblems:
	\begin{center} 
		$y'(t) = f(t, y(t)), y(t_0) = y_0, y: \realnumbers \rightarrow \realnumbers^d$
	\end{center}
	\textbf{Beispiele}:\\
	Die verschiedenen Runge-Kutta-Verfahren unterscheiden sich in erster Linie in der Konvergenzordnung R. Mit höherer Konvergenzordnung wächst die Zahl zu lösender Gleichungen schnell an, die Ergebnisse werden dafür jedoch genauer.
	\begin{itemize}
		\item Explizites Euler-Verfahren\hfill R = 1
		\item Verfahren von Heun\hfill R = 2
		\item Klassisches Runge-Kutta-Verfahren\hfill R = 4
	\end{itemize}
\end{document}
